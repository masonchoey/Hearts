Starting PPO Self-Play Training...
In self-play mode, the agent learns by playing against copies of itself.
This should lead to more sophisticated strategies over time.
Press Ctrl+C at any time to stop training early.
----------------------------------------------------------------------
2025-10-02 16:55:19,735	INFO worker.py:1927 -- Started a local Ray instance.
2025-10-02 16:55:20,306	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
/Users/masonchoey/Documents/GitHub/OpenSpiel-Hearts/.venv/lib/python3.13/site-packages/gymnasium/spaces/box.py:235: UserWarning: [33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64[0m
  gym.logger.warn(
/Users/masonchoey/Documents/GitHub/OpenSpiel-Hearts/.venv/lib/python3.13/site-packages/gymnasium/spaces/box.py:305: UserWarning: [33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64[0m
  gym.logger.warn(
/Users/masonchoey/Documents/GitHub/OpenSpiel-Hearts/.venv/lib/python3.13/site-packages/gymnasium/utils/passive_env_checker.py:134: UserWarning: [33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64[0m
  logger.warn(
/Users/masonchoey/Documents/GitHub/OpenSpiel-Hearts/.venv/lib/python3.13/site-packages/gymnasium/utils/passive_env_checker.py:158: UserWarning: [33mWARN: The obs returned by the `reset()` method is not within the observation space.[0m
  logger.warn(f"{pre} is not within the observation space.")
2025-10-02 16:55:20,366	INFO wandb.py:321 -- Already logged into W&B.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Configuration for experiment     PPO_2025-10-02_16-55-18   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Search algorithm                 BasicVariantGenerator     â”‚
â”‚ Scheduler                        FIFOScheduler             â”‚
â”‚ Number of trials                 1                         â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

View detailed results here: /Users/masonchoey/ray_results/PPO_2025-10-02_16-55-18
To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2025-10-02_16-55-18_425357_44033/artifacts/2025-10-02_16-55-20/PPO_2025-10-02_16-55-18/driver_artifacts`

Trial status: 1 PENDING
Current time: 2025-10-02 16:55:20. Total running time: 0s
Logical resource usage: 8.0/8 CPUs, 0/0 GPUs
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name                             status   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ PPO_hearts_env_self_play_40e76_00000   PENDING  â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(PPO pid=44087)[0m [2025-10-02 16:55:22,466 E 44087 566088] core_worker.cc:2740: Actor with class name: 'RolloutWorker' and ID: '24542e557f5d86b66aa66c0b01000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.
[36m(PPO pid=44087)[0m [2025-10-02 16:55:22,508 E 44087 566088] core_worker.cc:2740: Actor with class name: 'RolloutWorker' and ID: '343f7ccbc3b085a0bf6d299b01000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.
[36m(PPO pid=44087)[0m [2025-10-02 16:55:22,549 E 44087 566088] core_worker.cc:2740: Actor with class name: 'RolloutWorker' and ID: 'd0b08691f76f00fa57f7b2bf01000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.
[36m(PPO pid=44087)[0m [2025-10-02 16:55:22,591 E 44087 566088] core_worker.cc:2740: Actor with class name: 'RolloutWorker' and ID: 'e95db603b4d33773fd63adbe01000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.
[36m(PPO pid=44087)[0m [2025-10-02 16:55:22,633 E 44087 566088] core_worker.cc:2740: Actor with class name: 'RolloutWorker' and ID: 'ed626a1944ab6a7fdef605a101000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.
[36m(PPO pid=44087)[0m [2025-10-02 16:55:22,717 E 44087 566088] core_worker.cc:2740: Actor with class name: 'RolloutWorker' and ID: '773fabb355c6ac65885f2abd01000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.
[36m(PPO pid=44087)[0m [2025-10-02 16:55:22,773 E 44087 566088] core_worker.cc:2740: Actor with class name: 'RolloutWorker' and ID: 'b10fe13eed53c8794580c8ab01000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.
[36m(RolloutWorker pid=44095)[0m 2025-10-02 16:55:25,400	INFO catalog.py:407 -- Wrapping <class 'attention_model.AttentionMaskModel'> as None
[36m(RolloutWorker pid=44095)[0m 2025-10-02 16:55:25,436	INFO policy.py:1234 -- Policy (worker=1) running on CPU.
[36m(RolloutWorker pid=44095)[0m 2025-10-02 16:55:25,436	INFO torch_policy_v2.py:105 -- Found 0 visible cuda devices.
[36m(RolloutWorker pid=44095)[0m 2025-10-02 16:55:26,365	INFO util.py:118 -- Using connectors:
[36m(RolloutWorker pid=44095)[0m 2025-10-02 16:55:26,365	INFO util.py:119 --     AgentConnectorPipeline
[36m(RolloutWorker pid=44095)[0m         ObsPreprocessorConnector
[36m(RolloutWorker pid=44095)[0m         StateBufferConnector
[36m(RolloutWorker pid=44095)[0m         ViewRequirementAgentConnector
[36m(RolloutWorker pid=44095)[0m 2025-10-02 16:55:26,365	INFO util.py:120 --     ActionConnectorPipeline
[36m(RolloutWorker pid=44095)[0m         ConvertToNumpyConnector
[36m(RolloutWorker pid=44095)[0m         NormalizeActionsConnector
[36m(RolloutWorker pid=44095)[0m         ImmutableActionsConnector
[36m(PPO pid=44087)[0m 2025-10-02 16:55:26,635	INFO env_runner_group.py:320 -- Inferred observation/action spaces from remote worker (local worker has no env): {'default_policy': (Dict('action_mask': Box(0, 1, (52,), int8), 'observations': Box(0.0, 1.0, (5088,), float32)), Discrete(52)), '__env__': (Dict('action_mask': Box(0, 1, (52,), int8), 'observations': Box(0.0, 1.0, (5088,), float32)), Discrete(52))}
[36m(PPO pid=44087)[0m 2025-10-02 16:55:26,643	INFO policy.py:1234 -- Policy (worker=local) running on CPU.

Trial PPO_hearts_env_self_play_40e76_00000 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial PPO_hearts_env_self_play_40e76_00000 config                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ _disable_action_flattening                                           False â”‚
â”‚ _disable_execution_plan_api                                             -1 â”‚
â”‚ _disable_initialize_loss_from_dummy_batch                            False â”‚
â”‚ _disable_preprocessor_api                                            False â”‚
â”‚ _dont_auto_sync_env_runner_states                                    False â”‚
â”‚ _enable_rl_module_api                                                   -1 â”‚
â”‚ _env_to_module_connector                                                   â”‚
â”‚ _fake_gpus                                                           False â”‚
â”‚ _is_atari                                                                  â”‚
â”‚ _is_online                                                            True â”‚
â”‚ _learner_class                                                             â”‚
â”‚ _learner_connector                                                         â”‚
â”‚ _module_to_env_connector                                                   â”‚
â”‚ _prior_exploration_config                                                  â”‚
â”‚ _rl_module_spec                                                            â”‚
â”‚ _tf_policy_handles_more_than_one_loss                                False â”‚
â”‚ _torch_grad_scaler_class                                                   â”‚
â”‚ _torch_lr_scheduler_classes                                                â”‚
â”‚ _train_batch_size_per_learner                                              â”‚
â”‚ _use_msgpack_checkpoints                                             False â”‚
â”‚ _validate_config                                                      True â”‚
â”‚ action_mask_key                                                action_mask â”‚
â”‚ action_space                                                               â”‚
â”‚ actions_in_input_normalized                                          False â”‚
â”‚ add_default_connectors_to_env_to_module_pipeline                      True â”‚
â”‚ add_default_connectors_to_learner_pipeline                            True â”‚
â”‚ add_default_connectors_to_module_to_env_pipeline                      True â”‚
â”‚ always_attach_evaluation_results                                        -1 â”‚
â”‚ auto_wrap_old_gym_envs                                                  -1 â”‚
â”‚ batch_mode                                               truncate_episodes â”‚
â”‚ broadcast_env_runner_states                                           True â”‚
â”‚ broadcast_offline_eval_runner_states                                 False â”‚
â”‚ callbacks                                             ...s.RLlibCallback'> â”‚
â”‚ callbacks_on_algorithm_init                                                â”‚
â”‚ callbacks_on_checkpoint_loaded                                             â”‚
â”‚ callbacks_on_env_runners_recreated                                         â”‚
â”‚ callbacks_on_environment_created                                           â”‚
â”‚ callbacks_on_episode_created                                               â”‚
â”‚ callbacks_on_episode_end                                                   â”‚
â”‚ callbacks_on_episode_start                                                 â”‚
â”‚ callbacks_on_episode_step                                                  â”‚
â”‚ callbacks_on_evaluate_end                                                  â”‚
â”‚ callbacks_on_evaluate_offline_end                                          â”‚
â”‚ callbacks_on_evaluate_offline_start                                        â”‚
â”‚ callbacks_on_evaluate_start                                                â”‚
â”‚ callbacks_on_offline_eval_runners_recreated                                â”‚
â”‚ callbacks_on_sample_end                                                    â”‚
â”‚ callbacks_on_train_result                                                  â”‚
â”‚ checkpoint_trainable_policies_only                                   False â”‚
â”‚ clip_actions                                                         False â”‚
â”‚ clip_param                                                             0.2 â”‚
â”‚ clip_rewards                                                               â”‚
â”‚ compress_observations                                                False â”‚
â”‚ count_steps_by                                                   env_steps â”‚
â”‚ create_env_on_driver                                                 False â”‚
â”‚ create_local_env_runner                                               True â”‚
â”‚ custom_async_evaluation_function                                        -1 â”‚
â”‚ custom_eval_function                                                       â”‚
â”‚ dataset_num_iters_per_eval_runner                                        1 â”‚
â”‚ dataset_num_iters_per_learner                                              â”‚
â”‚ delay_between_env_runner_restarts_s                                    60. â”‚
â”‚ disable_env_checking                                                 False â”‚
â”‚ eager_max_retraces                                                      20 â”‚
â”‚ eager_tracing                                                         True â”‚
â”‚ enable_async_evaluation                                                 -1 â”‚
â”‚ enable_connectors                                                       -1 â”‚
â”‚ enable_env_runner_and_connector_v2                                   False â”‚
â”‚ enable_rl_module_and_learner                                         False â”‚
â”‚ enable_tf1_exec_eagerly                                              False â”‚
â”‚ entropy_coeff                                                         0.05 â”‚
â”‚ entropy_coeff_schedule                                                     â”‚
â”‚ env                                                   hearts_env_self_play â”‚
â”‚ env_runner_cls                                                             â”‚
â”‚ env_runner_health_probe_timeout_s                                      30. â”‚
â”‚ env_runner_restore_timeout_s                                         1800. â”‚
â”‚ env_task_fn                                                             -1 â”‚
â”‚ episode_lookback_horizon                                                 1 â”‚
â”‚ episodes_to_numpy                                                     True â”‚
â”‚ evaluation_auto_duration_max_env_steps_per_sample                     2000 â”‚
â”‚ evaluation_auto_duration_min_env_steps_per_sample                      100 â”‚
â”‚ evaluation_config/explore                                            False â”‚
â”‚ evaluation_duration                                                    300 â”‚
â”‚ evaluation_duration_unit                                          episodes â”‚
â”‚ evaluation_force_reset_envs_before_iteration                          True â”‚
â”‚ evaluation_interval                                                     15 â”‚
â”‚ evaluation_num_env_runners                                               0 â”‚
â”‚ evaluation_parallel_to_training                                      False â”‚
â”‚ evaluation_sample_timeout_s                                           120. â”‚
â”‚ exploration_config/type                                 StochasticSampling â”‚
â”‚ explore                                                               True â”‚
â”‚ export_native_model_files                                            False â”‚
â”‚ fake_sampler                                                         False â”‚
â”‚ framework                                                            torch â”‚
â”‚ gamma                                                                 0.99 â”‚
â”‚ grad_clip                                                              0.5 â”‚
â”‚ grad_clip_by                                                   global_norm â”‚
â”‚ gym_env_vectorize_mode                                                SYNC â”‚
â”‚ ignore_env_runner_failures                                           False â”‚
â”‚ ignore_final_observation                                             False â”‚
â”‚ ignore_offline_eval_runner_failures                                  False â”‚
â”‚ in_evaluation                                                        False â”‚
â”‚ input                                                              sampler â”‚
â”‚ input_compress_columns                                  ['obs', 'new_obs'] â”‚
â”‚ input_filesystem                                                           â”‚
â”‚ input_read_batch_size                                                      â”‚
â”‚ input_read_episodes                                                  False â”‚
â”‚ input_read_method                                             read_parquet â”‚
â”‚ input_read_sample_batches                                            False â”‚
â”‚ input_spaces_jsonable                                                 True â”‚
â”‚ keep_per_episode_custom_metrics                                      False â”‚
â”‚ kl_coeff                                                               0.2 â”‚
â”‚ kl_target                                                             0.01 â”‚
â”‚ lambda                                                                0.95 â”‚
â”‚ local_gpu_idx                                                            0 â”‚
â”‚ local_tf_session_args/inter_op_parallelism_threads                       8 â”‚
â”‚ local_tf_session_args/intra_op_parallelism_threads                       8 â”‚
â”‚ log_gradients                                                         True â”‚
â”‚ log_level                                                             INFO â”‚
â”‚ log_sys_usage                                                         True â”‚
â”‚ logger_config                                                              â”‚
â”‚ logger_creator                                                             â”‚
â”‚ lr                                                                  0.0002 â”‚
â”‚ lr_schedule                                                                â”‚
â”‚ materialize_data                                                     False â”‚
â”‚ materialize_mapped_data                                               True â”‚
â”‚ max_num_env_runner_restarts                                           1000 â”‚
â”‚ max_num_offline_eval_runner_restarts                                  1000 â”‚
â”‚ max_requests_in_flight_per_aggregator_actor                              3 â”‚
â”‚ max_requests_in_flight_per_env_runner                                    1 â”‚
â”‚ max_requests_in_flight_per_learner                                       3 â”‚
â”‚ max_requests_in_flight_per_offline_eval_runner                           1 â”‚
â”‚ merge_env_runner_states                                      training_only â”‚
â”‚ metrics_episode_collection_timeout_s                                   60. â”‚
â”‚ metrics_num_episodes_for_smoothing                                     100 â”‚
â”‚ min_sample_timesteps_per_iteration                                       0 â”‚
â”‚ min_time_s_per_iteration                                                   â”‚
â”‚ min_train_timesteps_per_iteration                                        0 â”‚
â”‚ minibatch_size                                                          32 â”‚
â”‚ model/_disable_action_flattening                                     False â”‚
â”‚ model/_disable_preprocessor_api                                      False â”‚
â”‚ model/_time_major                                                    False â”‚
â”‚ model/_use_default_native_models                                        -1 â”‚
â”‚ model/always_check_shapes                                            False â”‚
â”‚ model/attention_dim                                                     64 â”‚
â”‚ model/attention_head_dim                                                32 â”‚
â”‚ model/attention_init_gru_gate_bias                                     2.0 â”‚
â”‚ model/attention_memory_inference                                        50 â”‚
â”‚ model/attention_memory_training                                         50 â”‚
â”‚ model/attention_num_heads                                                1 â”‚
â”‚ model/attention_num_transformer_units                                    1 â”‚
â”‚ model/attention_position_wise_mlp_dim                                   32 â”‚
â”‚ model/attention_use_n_prev_actions                                       0 â”‚
â”‚ model/attention_use_n_prev_rewards                                       0 â”‚
â”‚ model/conv_activation                                                 relu â”‚
â”‚ model/conv_bias_initializer                                                â”‚
â”‚ model/conv_bias_initializer_config                                         â”‚
â”‚ model/conv_filters                                                         â”‚
â”‚ model/conv_kernel_initializer                                              â”‚
â”‚ model/conv_kernel_initializer_config                                       â”‚
â”‚ model/conv_transpose_bias_initializer                                      â”‚
â”‚ model/conv_transpose_bias_initializer_config                               â”‚
â”‚ model/conv_transpose_kernel_initializer                                    â”‚
â”‚ model/conv_transpose_kernel_initializer_config                             â”‚
â”‚ model/custom_action_dist                                                   â”‚
â”‚ model/custom_model                                    ...d_attention_model â”‚
â”‚ model/custom_preprocessor                                                  â”‚
â”‚ model/dim                                                               84 â”‚
â”‚ model/encoder_latent_dim                                                   â”‚
â”‚ model/fcnet_activation                                                tanh â”‚
â”‚ model/fcnet_bias_initializer                                               â”‚
â”‚ model/fcnet_bias_initializer_config                                        â”‚
â”‚ model/fcnet_hiddens                                             [256, 256] â”‚
â”‚ model/fcnet_weights_initializer                                            â”‚
â”‚ model/fcnet_weights_initializer_config                                     â”‚
â”‚ model/framestack                                                      True â”‚
â”‚ model/free_log_std                                                   False â”‚
â”‚ model/grayscale                                                      False â”‚
â”‚ model/log_std_clip_param                                              20.0 â”‚
â”‚ model/lstm_bias_initializer                                                â”‚
â”‚ model/lstm_bias_initializer_config                                         â”‚
â”‚ model/lstm_cell_size                                                   256 â”‚
â”‚ model/lstm_use_prev_action                                           False â”‚
â”‚ model/lstm_use_prev_action_reward                                       -1 â”‚
â”‚ model/lstm_use_prev_reward                                           False â”‚
â”‚ model/lstm_weights_initializer                                             â”‚
â”‚ model/lstm_weights_initializer_config                                      â”‚
â”‚ model/max_seq_len                                                       20 â”‚
â”‚ model/no_final_linear                                                False â”‚
â”‚ model/post_fcnet_activation                                           relu â”‚
â”‚ model/post_fcnet_bias_initializer                                          â”‚
â”‚ model/post_fcnet_bias_initializer_config                                   â”‚
â”‚ model/post_fcnet_hiddens                                                [] â”‚
â”‚ model/post_fcnet_weights_initializer                                       â”‚
â”‚ model/post_fcnet_weights_initializer_config                                â”‚
â”‚ model/use_attention                                                  False â”‚
â”‚ model/use_lstm                                                       False â”‚
â”‚ model/vf_share_layers                                                False â”‚
â”‚ model/zero_mean                                                       True â”‚
â”‚ normalize_actions                                                     True â”‚
â”‚ num_aggregator_actors_per_learner                                        0 â”‚
â”‚ num_consecutive_env_runner_failures_tolerance                          100 â”‚
â”‚ num_cpus_for_main_process                                                1 â”‚
â”‚ num_cpus_per_env_runner                                                  1 â”‚
â”‚ num_cpus_per_learner                                                  auto â”‚
â”‚ num_cpus_per_offline_eval_runner                                         1 â”‚
â”‚ num_env_runners                                                          7 â”‚
â”‚ num_envs_per_env_runner                                                  1 â”‚
â”‚ num_epochs                                                              20 â”‚
â”‚ num_gpus                                                                 0 â”‚
â”‚ num_gpus_per_env_runner                                                  0 â”‚
â”‚ num_gpus_per_learner                                                     0 â”‚
â”‚ num_gpus_per_offline_eval_runner                                         0 â”‚
â”‚ num_learners                                                             0 â”‚
â”‚ num_offline_eval_runners                                                 0 â”‚
â”‚ observation_filter                                                NoFilter â”‚
â”‚ observation_fn                                                             â”‚
â”‚ observation_space                                                          â”‚
â”‚ offline_data_class                                                         â”‚
â”‚ offline_eval_batch_size_per_runner                                     256 â”‚
â”‚ offline_eval_rl_module_inference_only                                False â”‚
â”‚ offline_eval_runner_class                                                  â”‚
â”‚ offline_eval_runner_health_probe_timeout_s                             30. â”‚
â”‚ offline_eval_runner_restore_timeout_s                                1800. â”‚
â”‚ offline_evaluation_duration                                              1 â”‚
â”‚ offline_evaluation_interval                                                â”‚
â”‚ offline_evaluation_parallel_to_training                              False â”‚
â”‚ offline_evaluation_timeout_s                                          120. â”‚
â”‚ offline_evaluation_type                                                    â”‚
â”‚ offline_loss_for_module_fn                                                 â”‚
â”‚ offline_sampling                                                     False â”‚
â”‚ ope_split_batch_by_episode                                            True â”‚
â”‚ output                                                                     â”‚
â”‚ output_compress_columns                                 ['obs', 'new_obs'] â”‚
â”‚ output_filesystem                                                          â”‚
â”‚ output_max_file_size                                              67108864 â”‚
â”‚ output_max_rows_per_file                                                   â”‚
â”‚ output_write_episodes                                                 True â”‚
â”‚ output_write_method                                          write_parquet â”‚
â”‚ output_write_remaining_data                                          False â”‚
â”‚ placement_strategy                                                    PACK â”‚
â”‚ policies/default_policy                               ...None, None, None) â”‚
â”‚ policies_to_train                                                          â”‚
â”‚ policy_map_cache                                                        -1 â”‚
â”‚ policy_map_capacity                                                    100 â”‚
â”‚ policy_mapping_fn                                     ...N at 0x152006160> â”‚
â”‚ policy_states_are_swappable                                          False â”‚
â”‚ postprocess_inputs                                                   False â”‚
â”‚ prelearner_buffer_class                                                    â”‚
â”‚ prelearner_class                                                           â”‚
â”‚ prelearner_module_synch_period                                          10 â”‚
â”‚ preprocessor_pref                                                 deepmind â”‚
â”‚ remote_env_batch_wait_ms                                                 0 â”‚
â”‚ remote_worker_envs                                                   False â”‚
â”‚ render_env                                                           False â”‚
â”‚ replay_sequence_length                                                     â”‚
â”‚ restart_failed_env_runners                                            True â”‚
â”‚ restart_failed_offline_eval_runners                                   True â”‚
â”‚ restart_failed_sub_environments                                      False â”‚
â”‚ rollout_fragment_length                                               auto â”‚
â”‚ sample_collector                                      ...leListCollector'> â”‚
â”‚ sample_timeout_s                                                       60. â”‚
â”‚ sampler_perf_stats_ema_coef                                                â”‚
â”‚ seed                                                                       â”‚
â”‚ sgd_minibatch_size                                                      -1 â”‚
â”‚ shuffle_batch_per_epoch                                               True â”‚
â”‚ shuffle_buffer_size                                                      0 â”‚
â”‚ simple_optimizer                                                        -1 â”‚
â”‚ sync_filters_on_rollout_workers_timeout_s                              10. â”‚
â”‚ synchronize_filters                                                     -1 â”‚
â”‚ tf_session_args/allow_soft_placement                                  True â”‚
â”‚ tf_session_args/device_count/CPU                                         1 â”‚
â”‚ tf_session_args/gpu_options/allow_growth                              True â”‚
â”‚ tf_session_args/inter_op_parallelism_threads                             2 â”‚
â”‚ tf_session_args/intra_op_parallelism_threads                             2 â”‚
â”‚ tf_session_args/log_device_placement                                 False â”‚
â”‚ torch_compile_learner                                                False â”‚
â”‚ torch_compile_learner_dynamo_backend                             aot_eager â”‚
â”‚ torch_compile_learner_dynamo_mode                                          â”‚
â”‚ torch_compile_learner_what_to_compile                 ...ile.FORWARD_TRAIN â”‚
â”‚ torch_compile_worker                                                 False â”‚
â”‚ torch_compile_worker_dynamo_backend                              aot_eager â”‚
â”‚ torch_compile_worker_dynamo_mode                                           â”‚
â”‚ torch_skip_nan_gradients                                             False â”‚
â”‚ train_batch_size                                                     12000 â”‚
â”‚ update_worker_filter_stats                                            True â”‚
â”‚ use_critic                                                            True â”‚
â”‚ use_gae                                                               True â”‚
â”‚ use_kl_loss                                                           True â”‚
â”‚ use_worker_filter_stats                                               True â”‚
â”‚ validate_env_runners_after_construction                               True â”‚
â”‚ validate_offline_eval_runners_after_construction                      True â”‚
â”‚ vf_clip_param                                                          10. â”‚
â”‚ vf_loss_coeff                                                           2. â”‚
â”‚ vf_share_layers                                                         -1 â”‚
â”‚ worker_cls                                                              -1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(PPO pid=44087)[0m 2025-10-02 16:55:26,957	INFO rollout_worker.py:1742 -- Built policy map: <PolicyMap lru-caching-capacity=100 policy-IDs=['default_policy']>
[36m(PPO pid=44087)[0m 2025-10-02 16:55:26,957	INFO rollout_worker.py:1743 -- Built preprocessor map: {'default_policy': None}
[36m(PPO pid=44087)[0m 2025-10-02 16:55:26,957	INFO rollout_worker.py:542 -- Built filter map: defaultdict(<class 'ray.rllib.utils.filter.NoFilter'>, {})
[36m(PPO pid=44087)[0m 2025-10-02 16:55:26,962	INFO policy.py:1234 -- Policy (worker=local) running on CPU.
[36m(PPO pid=44087)[0m 2025-10-02 16:55:26,965	INFO rollout_worker.py:1742 -- Built policy map: <PolicyMap lru-caching-capacity=100 policy-IDs=['default_policy']>
[36m(PPO pid=44087)[0m 2025-10-02 16:55:26,965	INFO rollout_worker.py:1743 -- Built preprocessor map: {'default_policy': None}
[36m(PPO pid=44087)[0m 2025-10-02 16:55:26,966	INFO rollout_worker.py:542 -- Built filter map: defaultdict(<class 'ray.rllib.utils.filter.NoFilter'>, {})
[36m(PPO pid=44087)[0m 2025-10-02 16:55:26,968	INFO env_runner_group.py:320 -- Inferred observation/action spaces from remote worker (local worker has no env): {'default_policy': (Dict('action_mask': Box(0, 1, (52,), int8), 'observations': Box(0.0, 1.0, (5088,), float32)), Discrete(52)), '__env__': (Dict('action_mask': Box(0, 1, (52,), int8), 'observations': Box(0.0, 1.0, (5088,), float32)), Discrete(52))}
[36m(PPO pid=44087)[0m Install gputil for GPU system monitoring.
[36m(RolloutWorker pid=44095)[0m 2025-10-02 16:55:27,017	INFO rollout_worker.py:671 -- Generating sample batch of size 1715
[36m(RolloutWorker pid=44095)[0m /Users/masonchoey/Documents/GitHub/OpenSpiel-Hearts/.venv/lib/python3.13/site-packages/numpy/_core/_methods.py:134: RuntimeWarning: overflow encountered in reduce
[36m(RolloutWorker pid=44095)[0m   ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)
[36m(RolloutWorker pid=44095)[0m 2025-10-02 16:55:30,584	INFO rollout_worker.py:713 -- Completed sample batch:
[36m(RolloutWorker pid=44095)[0m
[36m(RolloutWorker pid=44095)[0m { 'count': 1715,
[36m(RolloutWorker pid=44095)[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((1715, 52), dtype=float32, min=-3.4028234663852886e+38, max=0.518, mean=-inf),
[36m(RolloutWorker pid=44095)[0m                                           'action_logp': np.ndarray((1715,), dtype=float32, min=-3.41, max=0.0, mean=-1.21),
[36m(RolloutWorker pid=44095)[0m                                           'actions': np.ndarray((1715,), dtype=int32, min=0.0, max=51.0, mean=25.335),
[36m(RolloutWorker pid=44095)[0m                                           'advantages': np.ndarray((1715,), dtype=float32, min=-0.036, max=2.421, mean=0.499),
[36m(RolloutWorker pid=44095)[0m                                           'agent_index': np.ndarray((1715,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[36m(RolloutWorker pid=44095)[0m                                           'eps_id': np.ndarray((1715,), dtype=int64, min=2500128777101094.0, max=9.535233508584229e+17, mean=4.4834542236843955e+17),
[36m(RolloutWorker pid=44095)[0m                                           'infos': np.ndarray((1715,), dtype=object, head={}),
[36m(RolloutWorker pid=44095)[0m                                           'new_obs': np.ndarray((1715, 5140), dtype=float32, min=0.0, max=1.0, mean=0.019),
[36m(RolloutWorker pid=44095)[0m                                           'obs': np.ndarray((1715, 5140), dtype=float32, min=0.0, max=1.0, mean=0.019),
[36m(RolloutWorker pid=44095)[0m                                           'rewards': np.ndarray((1715,), dtype=float32, min=0.0, max=2.6, mean=0.035),
[36m(RolloutWorker pid=44095)[0m                                           't': np.ndarray((1715,), dtype=int64, min=0.0, max=63.0, mean=30.322),
[36m(RolloutWorker pid=44095)[0m                                           'terminateds': np.ndarray((1715,), dtype=bool, min=0.0, max=1.0, mean=0.016),
[36m(RolloutWorker pid=44095)[0m                                           'truncateds': np.ndarray((1715,), dtype=bool, min=0.0, max=0.0, mean=0.0),
[36m(RolloutWorker pid=44095)[0m                                           'unroll_id': np.ndarray((1715,), dtype=int64, min=1.0, max=55.0, mean=28.047),
[36m(RolloutWorker pid=44095)[0m                                           'value_targets': np.ndarray((1715,), dtype=float32, min=0.159, max=2.6, mean=0.689),
[36m(RolloutWorker pid=44095)[0m                                           'values_bootstrapped': np.ndarray((1715,), dtype=float32, min=0.0, max=0.222, mean=0.188),
[36m(RolloutWorker pid=44095)[0m                                           'vf_preds': np.ndarray((1715,), dtype=float32, min=0.165, max=0.222, mean=0.191)}},
[36m(RolloutWorker pid=44095)[0m   'type': 'MultiAgentBatch'}
[36m(RolloutWorker pid=44095)[0m
[36m(PPO pid=44087)[0m 2025-10-02 16:55:26,960	INFO catalog.py:407 -- Wrapping <class 'attention_model.AttentionMaskModel'> as None[32m [repeated 8x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(RolloutWorker pid=44100)[0m 2025-10-02 16:55:26,276	INFO policy.py:1234 -- Policy (worker=7) running on CPU.[32m [repeated 6x across cluster][0m
[36m(PPO pid=44087)[0m 2025-10-02 16:55:26,962	INFO torch_policy_v2.py:105 -- Found 0 visible cuda devices.[32m [repeated 8x across cluster][0m
[36m(PPO pid=44087)[0m 2025-10-02 16:55:30,882	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!
[36m(_WandbLoggingActor pid=44126)[0m wandb: WARNING `start_method` is deprecated and will be removed in a future version of wandb. This setting is currently non-functional and safely ignored.
[36m(PPO pid=44087)[0m 2025-10-02 16:55:26,965	INFO util.py:118 -- Using connectors:[32m [repeated 8x across cluster][0m
[36m(PPO pid=44087)[0m 2025-10-02 16:55:26,965	INFO util.py:119 --     AgentConnectorPipeline[32m [repeated 8x across cluster][0m
[36m(PPO pid=44087)[0m         ObsPreprocessorConnector[32m [repeated 8x across cluster][0m
[36m(PPO pid=44087)[0m         StateBufferConnector[32m [repeated 8x across cluster][0m
[36m(PPO pid=44087)[0m         ViewRequirementAgentConnector[32m [repeated 8x across cluster][0m
[36m(PPO pid=44087)[0m 2025-10-02 16:55:26,965	INFO util.py:120 --     ActionConnectorPipeline[32m [repeated 8x across cluster][0m
[36m(PPO pid=44087)[0m         ConvertToNumpyConnector[32m [repeated 8x across cluster][0m
[36m(PPO pid=44087)[0m         NormalizeActionsConnector[32m [repeated 8x across cluster][0m
[36m(PPO pid=44087)[0m         ImmutableActionsConnector[32m [repeated 8x across cluster][0m
[36m(_WandbLoggingActor pid=44126)[0m wandb: Currently logged in as: masonchoey (masonchoey-ucla) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[36m(_WandbLoggingActor pid=44126)[0m wandb: WARNING Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.
[36m(_WandbLoggingActor pid=44126)[0m wandb: Tracking run with wandb version 0.22.1
[36m(_WandbLoggingActor pid=44126)[0m wandb: Run data is saved locally in /private/tmp/ray/session_2025-10-02_16-55-18_425357_44033/artifacts/2025-10-02_16-55-20/PPO_2025-10-02_16-55-18/driver_artifacts/PPO_hearts_env_self_play_40e76_00000_0_2025-10-02_16-55-20/wandb/run-20251002_165532-40e76_00000
[36m(_WandbLoggingActor pid=44126)[0m wandb: Run `wandb offline` to turn off syncing.
[36m(_WandbLoggingActor pid=44126)[0m wandb: Syncing run hearts_training_run_2025-10-02-16-55-18
[36m(_WandbLoggingActor pid=44126)[0m wandb: â­ï¸ View project at https://wandb.ai/masonchoey-ucla/hearts-ppo-selfplay_runs
[36m(_WandbLoggingActor pid=44126)[0m wandb: ğŸš€ View run at https://wandb.ai/masonchoey-ucla/hearts-ppo-selfplay_runs/runs/40e76_00000

Trial status: 1 RUNNING
Current time: 2025-10-02 16:55:50. Total running time: 30s
Logical resource usage: 8.0/8 CPUs, 0/0 GPUs
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name                             status   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ PPO_hearts_env_self_play_40e76_00000   RUNNING  â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
Trial status: 1 RUNNING
Current time: 2025-10-02 16:56:20. Total running time: 1min 0s
Logical resource usage: 8.0/8 CPUs, 0/0 GPUs
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name                             status       iter     total time (s)      ts     num_healthy_workers     ...anding_async_reqs     ...e_worker_restarts     ...ent_steps_sampled â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ PPO_hearts_env_self_play_40e76_00000   RUNNING         1             36.871   12000                       7                        0                        0                    12000 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
Trial status: 1 RUNNING
Current time: 2025-10-02 16:56:50. Total running time: 1min 30s
Logical resource usage: 8.0/8 CPUs, 0/0 GPUs
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name                             status       iter     total time (s)      ts     num_healthy_workers     ...anding_async_reqs     ...e_worker_restarts     ...ent_steps_sampled â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ PPO_hearts_env_self_play_40e76_00000   RUNNING         2            72.2479   24000                       7                        0                        0                    24000 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
Trial status: 1 RUNNING
Current time: 2025-10-02 16:57:20. Total running time: 2min 0s
Logical resource usage: 8.0/8 CPUs, 0/0 GPUs
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name                             status       iter     total time (s)      ts     num_healthy_workers     ...anding_async_reqs     ...e_worker_restarts     ...ent_steps_sampled â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ PPO_hearts_env_self_play_40e76_00000   RUNNING         3            110.916   36000                       7                        0                        0                    36000 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
Trial status: 1 RUNNING
Current time: 2025-10-02 16:57:50. Total running time: 2min 30s
Logical resource usage: 8.0/8 CPUs, 0/0 GPUs
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name                             status       iter     total time (s)      ts     num_healthy_workers     ...anding_async_reqs     ...e_worker_restarts     ...ent_steps_sampled â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ PPO_hearts_env_self_play_40e76_00000   RUNNING         3            110.916   36000                       7                        0                        0                    36000 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
2025-10-02 16:58:18,395	WARNING tune.py:219 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip.
2025-10-02 16:58:18,400	INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/masonchoey/ray_results/PPO_2025-10-02_16-55-18' in 0.0038s.
Trial status: 1 RUNNING
Current time: 2025-10-02 16:58:18. Total running time: 2min 58s
Logical resource usage: 8.0/8 CPUs, 0/0 GPUs
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name                             status       iter     total time (s)      ts     num_healthy_workers     ...anding_async_reqs     ...e_worker_restarts     ...ent_steps_sampled â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ PPO_hearts_env_self_play_40e76_00000   RUNNING         4            151.454   48000                       7                        0                        0                    48000 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

======================================================================
Self-Play Training interrupted by user (Ctrl+C)
Checkpoints have been saved and can be found in the Ray results directory.
You can resume training later using:
  --resume <path_to_specific_checkpoint>
  --resume-from-latest <path_to_results_directory>
